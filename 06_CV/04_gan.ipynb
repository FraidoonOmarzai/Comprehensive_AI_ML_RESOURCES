{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network (GAN) In Depth\n",
    "\n",
    "**Contents**\n",
    "\n",
    "- Introduction\n",
    "- GAN\n",
    "- Deep Convolutional GAN\n",
    "- Conditional And Unconditional GAN\n",
    "- Style GAN\n",
    "- Cycle GAN\n",
    "- Evaluation Metrics\n",
    "- Pros And Cons\n",
    "\n",
    "### Introduction\n",
    "\n",
    "- Deep learning models can be broadly classified into generative and discriminative models.\n",
    "\n",
    "![alt text](../Images/cv/gan.png)\n",
    "\n",
    "### Discriminative Models\n",
    "\n",
    "Discriminative models learn the decision boundary between different classes. They focus on modeling the conditional probability P(y∣x) where y is the label and x is the input data. These models are used primarily for classification tasks.\n",
    "\n",
    "- Draw boundaries in the data space\n",
    "- X → Y\n",
    "- P(Y | X)\n",
    "1. **Logistic Regression**: A simple linear model used for binary classification tasks. It uses the logistic function to predict probabilities.\n",
    "2. **Support Vector Machines (SVM)**: A classification method that finds the hyperplane which best separates different classes. It can be extended to non-linear classification using kernel functions.\n",
    "3. **Decision Trees**: A model that splits the data into subsets based on the value of input features, creating a tree-like structure for decision-making.\n",
    "4. **Random Forests**: An ensemble method that builds multiple decision trees and merges them to get a more accurate and stable prediction.\n",
    "5. **Neural Networks**: Including Convolutional Neural Networks (CNNs) for image data, Recurrent Neural Networks (RNNs) for sequential data, and feedforward neural networks for various types of data.\n",
    "6. **Linear Discriminant Analysis (LDA)**: A method used for dimensionality reduction while preserving as much of the class discriminatory information as possible.\n",
    "\n",
    "### Generative Models\n",
    "\n",
    "Generative models learn the distribution of the data itself. They model the joint probability P(x,y) and can generate new data samples that are similar to the training data.\n",
    "\n",
    "- Try to model how data is placed throughout the space\n",
    "- E, Y → X\n",
    "- E: noise, Y:class, X: Features\n",
    "1. **Gaussian Mixture Models (GMM)**: A probabilistic model that assumes all the data points are generated from a mixture of several Gaussian distributions with unknown parameters.\n",
    "2. **Hidden Markov Models (HMM)**: A statistical model that represents systems that transition from one state to another with a certain probability, useful for time-series data.\n",
    "3. **Variational Autoencoders (VAE)**: A type of autoencoder that uses variational methods to approximate the distribution of the data and generate new data points.\n",
    "4. **Generative Adversarial Networks (GANs)**: Comprising two networks, a generator and a discriminator, that are trained together. The generator tries to create data that is indistinguishable from real data, while the discriminator tries to differentiate between real and generated data.\n",
    "5. **Boltzmann Machines**: Stochastic neural networks that can learn the underlying probability distribution of a dataset.\n",
    "6. **Restricted Boltzmann Machines (RBM)**: A simplified version of Boltzmann Machines with restrictions that make them easier to train.\n",
    "7. **Autoregressive Models**: Models like PixelRNN and PixelCNN that generate data points sequentially, with each new data point conditioned on the previous ones.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "- **Objective**: Discriminative models aim to find the decision boundary between classes, while generative models aim to understand and replicate the underlying data distribution.\n",
    "- **Probability Modeling**: Discriminative models estimate P(y∣x), while generative models estimate P(x,y) or P(x).\n",
    "- **Use Cases**: Discriminative models are used for tasks like classification and regression, whereas generative models are used for tasks like image and text generation, anomaly detection, and semi-supervised learning.\n",
    "\n",
    "### **GAN**\n",
    "\n",
    "Generative Adversarial Networks (GANs) are a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in 2014. They consist of two neural networks, the generator and the discriminator, which are trained simultaneously by competing against each other. This adversarial process allows GANs to produce highly realistic synthetic data.\n",
    "\n",
    "![alt text](../Images/cv/gan2.png)\n",
    "\n",
    "**Components:**\n",
    "\n",
    "1. **Generator (G):**\n",
    "    - Purpose: To generate fake data resembling the real data.\n",
    "    - Function: Takes a random noise vector (latent space vector) as input and transforms it into synthetic data.\n",
    "2. **Discriminator (D):**\n",
    "    - Purpose: To distinguish between real and fake data.\n",
    "    - Function: Takes an input (either real data or synthetic data from the generator) and outputs a probability indicating whether the input is real or fake.\n",
    "\n",
    "**Training Process:**\n",
    "\n",
    "1. **Adversarial Training:**\n",
    "    - The generator creates fake data to fool the discriminator.\n",
    "    - The discriminator learns to better distinguish between real and fake data.\n",
    "    - The training alternates between updating the discriminator to improve its classification accuracy and updating the generator to produce more realistic data.\n",
    "2. **Loss Functions:**\n",
    "    - **Discriminator Loss:** Measures how well the discriminator can distinguish real from fake data.\n",
    "    - **Generator Loss:** Measures how well the generator can fool the discriminator.\n",
    "\n",
    "**Variants of GANs:**\n",
    "\n",
    "1. **Conditional GANs (cGANs):** GANs conditioned on some extra information, such as class labels.\n",
    "2. **CycleGAN:** Enables image-to-image translation without requiring paired examples.\n",
    "3. **DCGAN:** Uses deep convolutional networks for both the generator and the discriminator.\n",
    "4. **Wasserstein GAN (WGAN):** Uses a different loss function to improve training stability and address issues like mode collapse.\n",
    "\n",
    "### DCGAN Architecture\n",
    "\n",
    "DCGANs apply specific architectural guidelines to standard GANs, particularly using convolutional layers to improve the generation of high-quality images.\n",
    "\n",
    "![alt text](../Images/cv/gan1.png)\n",
    "\n",
    "**key components**\n",
    "\n",
    "1. **Convolutional Layers**: Replace fully connected layers with convolutional layers in both the generator and discriminator to leverage spatial hierarchies and local dependencies in images.\n",
    "2. **Transposed Convolutional Layers**: Use transposed (or fractionally-strided) convolutions in the generator to upsample the input noise vector z into larger images.\n",
    "3. **Batch Normalization**: Apply batch normalization in both the generator and discriminator to stabilize training and help with gradient flow.\n",
    "4. **Leaky ReLU Activation**: Use Leaky ReLU activation in the discriminator, except for the output layer, where a sigmoid activation is used to output probabilities. The generator typically uses ReLU activations, except for the output layer, which uses a Tanh activation to produce normalized image pixel values.\n",
    "5. **No Pooling Layers**: Avoid using pooling layers. Instead, use strided convolutions in the discriminator and fractional-strided convolutions in the generator to achieve downsampling and upsampling, respectively.\n",
    "\n",
    "### DCGAN Generator Network\n",
    "\n",
    "The generator network in a DCGAN consists of a series of transposed convolutional layers, with each layer progressively increasing the spatial dimensions of the input noise vector while decreasing the number of feature maps. The final layer outputs a synthetic image with the same dimensions as the real images in the dataset.\n",
    "\n",
    "Example of a generator architecture:\n",
    "\n",
    "```yaml\n",
    "Input: Random noise vector z\n",
    "Fully connected layer, reshaped to (4x4x1024)\n",
    "Transposed Convolution: 4x4, stride 2, padding 1, output size (8x8x512)\n",
    "BatchNorm + ReLU\n",
    "Transposed Convolution: 4x4, stride 2, padding 1, output size (16x16x256)\n",
    "BatchNorm + ReLU\n",
    "Transposed Convolution: 4x4, stride 2, padding 1, output size (32x32x128)\n",
    "BatchNorm + ReLU\n",
    "Transposed Convolution: 4x4, stride 2, padding 1, output size (64x64x3)\n",
    "Tanh Activation\n",
    "Output: Synthetic image of size 64x64x3\n",
    "```\n",
    "\n",
    "### DCGAN Discriminator Network\n",
    "\n",
    "The discriminator network in a DCGAN consists of a series of convolutional layers, with each layer progressively decreasing the spatial dimensions of the input image while increasing the number of feature maps. The final layer outputs a single value representing the probability that the input image is real.\n",
    "\n",
    "Example of a discriminator architecture:\n",
    "\n",
    "```vbnet\n",
    "Input: Image of size 64x64x3\n",
    "Convolution: 4x4, stride 2, padding 1, output size (32x32x128)\n",
    "Leaky ReLU\n",
    "Convolution: 4x4, stride 2, padding 1, output size (16x16x256)\n",
    "BatchNorm + Leaky ReLU\n",
    "Convolution: 4x4, stride 2, padding 1, output size (8x8x512)\n",
    "BatchNorm + Leaky ReLU\n",
    "Convolution: 4x4, stride 2, padding 1, output size (4x4x1024)\n",
    "BatchNorm + Leaky ReLU\n",
    "Fully connected layer\n",
    "Sigmoid Activation\n",
    "Output: Probability that the input image is real\n",
    "```\n",
    "\n",
    "### Training DCGANs\n",
    "\n",
    "Training DCGANs involves alternately updating the discriminator and the generator. The discriminator is updated to maximize the likelihood of correctly classifying real and fake images, while the generator is updated to minimize the likelihood of the discriminator correctly classifying its generated images.\n",
    "\n",
    "**Training Steps:**\n",
    "\n",
    "1. Sample a batch of real images from the training dataset.\n",
    "2. Sample a batch of random noise vectors.\n",
    "3. Generate fake images using the generator.\n",
    "4. Train the discriminator on the combined batch of real and fake images.\n",
    "5. Sample another batch of random noise vectors.\n",
    "6. Train the generator using the gradients from the discriminator’s classification of fake images.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Improved image generation quality due to the use of convolutional layers.\n",
    "- Stability in training provided by architectural guidelines like batch normalization and avoiding pooling layers.\n",
    "- Ability to learn unsupervised representations useful for downstream tasks.\n",
    "\n",
    "**Challenges:**\n",
    "\n",
    "- Training can be unstable and sensitive to hyperparameters.\n",
    "- Mode collapse, where the generator produces a limited variety of images.\n",
    "- Difficulty in generating high-resolution images.\n",
    "\n",
    "### Unconditional GANs\n",
    "\n",
    "Unconditional GANs generate data samples without any conditioning information. The generator network receives only a random noise vector as input and produces data that should mimic the real data distribution. The discriminator network tries to distinguish between real and fake data samples.\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "1. **Generator (G):** Takes a random noise vector z and generates synthetic data.\n",
    "2. **Discriminator (D):** Takes either real data or synthetic data from the generator and classifies it as real or fake.\n",
    "\n",
    "**Training Process:**\n",
    "\n",
    "1. Sample noise vectors from a predefined distribution (e.g., normal distribution).\n",
    "2. Generate synthetic data from the noise vectors using the generator.\n",
    "3. Train the discriminator with both real and generated data.\n",
    "4. Update the generator to improve its ability to produce data that the discriminator misclassifies as real.\n",
    "\n",
    "**Example:**\n",
    "Unconditional GANs can generate random images from a dataset like MNIST (handwritten digits) without any labels or additional context.\n",
    "\n",
    "### Conditional GANs (cGANs)\n",
    "\n",
    "Conditional GANs generate data samples conditioned on some additional information or labels. The generator and discriminator both receive extra information (e.g., class labels, text descriptions) along with the noise vector and data samples, respectively. This allows for more controlled generation of data.\n",
    "\n",
    "![alt text](../Images/cv/gan3.png)\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "1. **Generator (G):** Takes a random noise vector z and a condition vector y (e.g., class labels) to generate synthetic data corresponding to the condition.\n",
    "2. **Discriminator (D):** Takes both the condition vector y and either real or synthetic data and classifies the pair as real or fake.\n",
    "\n",
    "**Training Process:**\n",
    "\n",
    "1. Sample noise vectors and condition vectors from predefined distributions.\n",
    "2. Generate synthetic data conditioned on the condition vectors using the generator.\n",
    "3. Train the discriminator with pairs of condition vectors and real or generated data.\n",
    "4. Update the generator to improve its ability to produce data that matches the condition vectors and fools the discriminator.\n",
    "\n",
    "**Example:**\n",
    "Conditional GANs can generate specific types of images, such as handwritten digits labeled \"5\" from the MNIST dataset or specific classes of images from the CIFAR-10 dataset.\n",
    "\n",
    "### Differences Between Unconditional and Conditional GANs\n",
    "\n",
    "1. **Input to Generator:**\n",
    "    - **Unconditional GAN:** Random noise vector z.\n",
    "    - **Conditional GAN:** Random noise vector z and condition vector y.\n",
    "2. **Input to Discriminator:**\n",
    "    - **Unconditional GAN:** Data sample (either real or generated).\n",
    "    - **Conditional GAN:** Data sample and condition vector y.\n",
    "3. **Output Control:**\n",
    "    - **Unconditional GAN:** Generates data without control over the specific characteristics of the output.\n",
    "    - **Conditional GAN:** Generates data with control over specific characteristics based on the condition vector y.\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "**Unconditional GAN Example (Pseudocode):**\n",
    "\n",
    "```python\n",
    "# Generator\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Define layers\n",
    "\n",
    "    def forward(self, z):\n",
    "        # Forward pass\n",
    "        return generated_data\n",
    "\n",
    "# Discriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Define layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        return validity\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data in dataloader:\n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(batch_size, noise_dim)\n",
    "        fake_data = generator(noise)\n",
    "        d_loss = compute_d_loss(discriminator, real_data, fake_data)\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        noise = torch.randn(batch_size, noise_dim)\n",
    "        fake_data = generator(noise)\n",
    "        g_loss = compute_g_loss(discriminator, fake_data)\n",
    "        g_optimizer.step()\n",
    "```\n",
    "\n",
    "**Conditional GAN Example (Pseudocode):**\n",
    "\n",
    "```python\n",
    "# Generator\n",
    "class ConditionalGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConditionalGenerator, self).__init__()\n",
    "        # Define layers\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        # Forward pass with condition\n",
    "        return generated_data\n",
    "\n",
    "# Discriminator\n",
    "class ConditionalDiscriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConditionalDiscriminator, self).__init__()\n",
    "        # Define layers\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # Forward pass with condition\n",
    "        return validity\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for real_data, labels in dataloader:\n",
    "        # Train Discriminator\n",
    "        noise = torch.randn(batch_size, noise_dim)\n",
    "        fake_data = generator(noise, labels)\n",
    "        d_loss = compute_d_loss(discriminator, real_data, fake_data, labels)\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train Generator\n",
    "        noise = torch.randn(batch_size, noise_dim)\n",
    "        fake_data = generator(noise, labels)\n",
    "        g_loss = compute_g_loss(discriminator, fake_data, labels)\n",
    "        g_optimizer.step()\n",
    "```\n",
    "\n",
    "### StyleGAN\n",
    "\n",
    "- StyleGAN is an advanced type of Generative Adversarial Network (GAN) introduced by NVIDIA researchers in a series of papers, notably \"A Style-Based Generator Architecture for Generative Adversarial Networks\" (2018) and its subsequent improvements.\n",
    "- StyleGAN introduces a style-based generator that separates the high-level attributes of an image (such as pose and identity) from stochastic variations (such as freckles and hair). This separation allows for more intuitive and fine-grained control over the generated images.\n",
    "\n",
    "![alt text](../Images/cv/gan4.png)\n",
    "\n",
    "### Main Components\n",
    "\n",
    "1. **Progressive Growing:** Similar to Progressive GANs, StyleGANs are trained using a progressive growing technique, where the resolution of the generated images gradually increases during training. This approach helps stabilize training and allows the model to learn finer details as it progresses.\n",
    "2. **Noise Mapping Network:** The noise mapping network in StyleGAN, through the mapping network and the use of the W space, introduces a layer of control that allows for the disentanglement of different attributes in the generated images. This separation of style control at various layers of the generator enables StyleGAN to produce high-quality images with precise control over their appearance, making it a powerful tool for various applications, from creative content generation to facial attribute editing.\n",
    "3. **Adaptive Instance Normalization (AdaIN):** Modulates the feature maps at each convolutional layer using the style vectors derived from w. This allows different styles to be applied at different layers, controlling various aspects of the image generation process.\n",
    "\n",
    "- **Mapping Network:** A series of fully connected layers that transform the initial latent code z (sampled from a Gaussian distribution) into an intermediate latent space w.\n",
    "\n",
    "### StyleGAN Architecture\n",
    "\n",
    "The StyleGAN generator consists of several key components:\n",
    "\n",
    "1. **Mapping Network:**\n",
    "    - Transforms the latent code z into w through a series of fully connected layers.\n",
    "    - The intermediate latent space W enables more control over the styles applied at different layers.\n",
    "2. **Synthesis Network:**\n",
    "    - Uses AdaIN to apply styles to the feature maps at each convolutional layer.\n",
    "    - Incorporates stochastic noise inputs to add fine details to the images.\n",
    "    - Progressive growing is used to incrementally increase the resolution of generated images.\n",
    "\n",
    "### Implementation Example (Pseudocode)\n",
    "\n",
    "```python\n",
    "# Mapping Network\n",
    "class MappingNetwork(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim):\n",
    "        super(MappingNetwork, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(w_dim, w_dim),\n",
    "            nn.ReLU(),\n",
    "            # More layers as needed\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.fc(z)\n",
    "        return w\n",
    "\n",
    "# Synthesis Network\n",
    "class SynthesisNetwork(nn.Module):\n",
    "    def __init__(self, w_dim, img_channels):\n",
    "        super(SynthesisNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(w_dim, 512, 3, padding=1)\n",
    "        self.adain1 = AdaIN()\n",
    "        self.conv2 = nn.Conv2d(512, 256, 3, padding=1)\n",
    "        self.adain2 = AdaIN()\n",
    "        # More layers as needed\n",
    "\n",
    "    def forward(self, w):\n",
    "        x = self.conv1(w)\n",
    "        x = self.adain1(x, w)\n",
    "        x = self.conv2(x)\n",
    "        x = self.adain2(x, w)\n",
    "        # More layers as needed\n",
    "        return x\n",
    "\n",
    "# StyleGAN Model\n",
    "class StyleGAN(nn.Module):\n",
    "    def __init__(self, z_dim, w_dim, img_channels):\n",
    "        super(StyleGAN, self).__init__()\n",
    "        self.mapping = MappingNetwork(z_dim, w_dim)\n",
    "        self.synthesis = SynthesisNetwork(w_dim, img_channels)\n",
    "\n",
    "    def forward(self, z):\n",
    "        w = self.mapping(z)\n",
    "        img = self.synthesis(w)\n",
    "        return img\n",
    "\n",
    "# Training loop (simplified)\n",
    "for epoch in range(num_epochs):\n",
    "    for z in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        generated_images = stylegan(z)\n",
    "        loss = compute_loss(generated_images, real_images)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "```\n",
    "\n",
    "### CycleGAN\n",
    "\n",
    "CycleGAN (Cycle-Consistent Generative Adversarial Network) is a type of GAN designed for unpaired image-to-image translation. Unlike other models that require paired training data, CycleGAN can learn to translate images from one domain to another using unpaired datasets. This ability is particularly useful when paired datasets are unavailable or difficult to obtain.\n",
    "\n",
    "![alt text](../Images/cv/gan5.png)\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "### 1. Unpaired Image-to-Image Translation\n",
    "\n",
    "Traditional image-to-image translation methods require paired datasets, where each image in one domain has a corresponding image in another domain. CycleGAN eliminates this requirement by using unpaired datasets, learning the translation between domains solely based on the images within each domain.\n",
    "\n",
    "### 2. Cycle Consistency\n",
    "\n",
    "The core idea behind CycleGAN is cycle consistency. This concept ensures that an image translated from domain X to domain Y and then back to domain X should be the same as the original image. This cycle consistency loss helps the model learn mappings that are more accurate and preserve the content of the images.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "CycleGAN consists of two pairs of generator and discriminator networks:\n",
    "\n",
    "1. **Generators (G and F):**\n",
    "    - G: X→Y: Translates images from domain X to domain Y.\n",
    "    - F: Y→X: Translates images from domain Y to domain X.\n",
    "2. **Discriminators (D_X and D_Y):**\n",
    "    - D_X: Distinguishes between real images from domain X and fake images generated by F from domain Y.\n",
    "    - D_Y: Distinguishes between real images from domain Y and fake images generated by G from domain X.\n",
    "\n",
    "### Loss Functions\n",
    "\n",
    "CycleGAN uses three types of losses to train the networks:\n",
    "\n",
    "1. **Adversarial Loss:**\n",
    "    - Ensures that the generated images are indistinguishable from real images in the target domain.\n",
    "2. **Cycle Consistency Loss:**\n",
    "    - Ensures that an image can be translated to the other domain and back without significant changes.\n",
    "3. **Identity Loss:**\n",
    "    - Ensures that the generator doesn't alter images that are already in the target domain.\n",
    "\n",
    "### Implementation Example (Pseudocode)\n",
    "\n",
    "```python\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        # Define the generator network architecture\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        return output\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        # Define the discriminator network architecture\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass\n",
    "        return output\n",
    "\n",
    "# Instantiate the networks\n",
    "G = Generator()  # X -> Y\n",
    "F = Generator()  # Y -> X\n",
    "D_X = Discriminator()  # Discriminator for domain X\n",
    "D_Y = Discriminator()  # Discriminator for domain Y\n",
    "\n",
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_F = torch.optim.Adam(F.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_X = torch.optim.Adam(D_X.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "optimizer_D_Y = torch.optim.Adam(D_Y.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "\n",
    "# Training loop (simplified)\n",
    "for epoch in range(num_epochs):\n",
    "    for real_X, real_Y in dataloader:\n",
    "        # Train Generators G and F\n",
    "        optimizer_G.zero_grad()\n",
    "        optimizer_F.zero_grad()\n",
    "        fake_Y = G(real_X)\n",
    "        recon_X = F(fake_Y)\n",
    "        fake_X = F(real_Y)\n",
    "        recon_Y = G(fake_X)\n",
    "\n",
    "        # Calculate cycle consistency loss\n",
    "        loss_cyc = cycle_consistency_loss(real_X, recon_X, real_Y, recon_Y)\n",
    "\n",
    "        # Calculate adversarial loss\n",
    "        loss_GAN_G = adversarial_loss(D_Y, fake_Y)\n",
    "        loss_GAN_F = adversarial_loss(D_X, fake_X)\n",
    "\n",
    "        # Calculate identity loss\n",
    "        loss_id = identity_loss(G, F, real_X, real_Y)\n",
    "\n",
    "        # Total generator loss\n",
    "        loss_G_F = loss_GAN_G + loss_GAN_F + lambda_cyc * loss_cyc + gamma_id * loss_id\n",
    "        loss_G_F.backward()\n",
    "        optimizer_G.step()\n",
    "        optimizer_F.step()\n",
    "\n",
    "        # Train Discriminators D_X and D_Y\n",
    "        optimizer_D_X.zero_grad()\n",
    "        optimizer_D_Y.zero_grad()\n",
    "        loss_D_X = discriminator_loss(D_X, real_X, fake_X.detach())\n",
    "        loss_D_Y = discriminator_loss(D_Y, real_Y, fake_Y.detach())\n",
    "        loss_D_X.backward()\n",
    "        loss_D_Y.backward()\n",
    "        optimizer_D_X.step()\n",
    "        optimizer_D_Y.step()\n",
    "```\n",
    "\n",
    "### GAN Evaluation\n",
    "\n",
    "Evaluating the performance of GANs (Generative Adversarial Networks) is crucial to understanding how well the generator and discriminator networks perform. Various metrics are used to assess the quality and diversity of the generated images, as well as the convergence of the GAN training process.\n",
    "\n",
    "### 1. Inception Score (IS)\n",
    "\n",
    "**Description:**\n",
    "The Inception Score evaluates the quality and diversity of generated images by using a pre-trained Inception v3 network. It considers both the confidence of the classifier in its predictions and the diversity of the generated samples.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "IS(G)=exp(E_{x∼pG}[D_{KL}(p(y∣x)∥p(y))])\n",
    "$$\n",
    "\n",
    "- where p(y∣x) is the conditional label distribution predicted by the Inception model for a generated image x, and p(y) is the marginal distribution.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Easy to compute.\n",
    "- Reflects both quality and diversity of images.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Sensitive to the choice of the pre-trained model.\n",
    "- May not always correlate well with human judgment.\n",
    "\n",
    "### 2. Frechet Inception Distance (FID)\n",
    "\n",
    "**Description:**\n",
    "The FID score measures the distance between the feature distributions of real and generated images. It uses the activations of a pre-trained Inception v3 model's intermediate layers to calculate the mean and covariance of the features.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$$\n",
    "FID=∥μ_r−μ_g∥2+T_r(Σ_r+Σ_g−2(Σ_rΣ_g)^{1/2})\n",
    "$$\n",
    "\n",
    "- where (μr,Σr) and (μg,Σg) are the mean and covariance of the features of real and generated images, respectively.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Considers both the quality and diversity of images.\n",
    "- More robust and correlates better with human judgment than IS.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Computationally expensive.\n",
    "- Sensitive to the choice of pre-trained model and the number of samples.\n",
    "\n",
    "### 3. Precision and Recall for Distributions\n",
    "\n",
    "**Description:**\n",
    "Precision and recall metrics adapted for generative models measure the quality and diversity of the generated samples. Precision evaluates how much of the generated data falls within the real data distribution, while recall measures how well the real data distribution is covered by the generated data.\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Provides a clear distinction between quality (precision) and diversity (recall).\n",
    "- More informative when used together.\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Computationally intensive.\n",
    "- Requires careful tuning of thresholds.\n",
    "\n",
    "### Pros of GANs\n",
    "\n",
    "1. **High-Quality Data Generation:**\n",
    "    - GANs are capable of generating high-resolution and highly realistic images, videos, and other types of data. The quality of the generated data is often superior to that produced by other generative models.\n",
    "2. **Unsupervised Learning:**\n",
    "    - GANs can learn to generate data without requiring labeled training data. This is particularly useful in situations where labeled data is scarce or expensive to obtain.\n",
    "3. **Flexibility:**\n",
    "    - GANs can be applied to a wide range of tasks beyond image generation, including image-to-image translation, text-to-image synthesis, super-resolution, inpainting, and more.\n",
    "4. **Creative Applications:**\n",
    "    - GANs have been used in various creative fields such as art, music, and design, enabling the creation of novel and innovative works that were previously difficult or impossible to produce.\n",
    "5. **Data Augmentation:**\n",
    "    - GANs can generate synthetic data to augment existing datasets, which can be beneficial for training machine learning models, especially in scenarios where real data is limited.\n",
    "6. **Learning Features:**\n",
    "    - GANs can learn complex data distributions and capture intricate features in the data, which can be useful for downstream tasks such as feature extraction and representation learning.\n",
    "\n",
    "### Cons of GANs\n",
    "\n",
    "1. **Training Instability:**\n",
    "    - Training GANs is notoriously difficult and unstable. The process often suffers from issues such as mode collapse, where the generator produces a limited variety of outputs, and vanishing gradients, where the discriminator becomes too strong, hindering the generator's learning.\n",
    "2. **Resource Intensive:**\n",
    "    - GANs require significant computational resources for training, including high-performance GPUs and substantial amounts of memory. The training process can be very slow, especially for large and complex models.\n",
    "3. **Sensitive to Hyperparameters:**\n",
    "    - The performance of GANs is highly sensitive to the choice of hyperparameters, such as learning rates, network architectures, and batch sizes. Tuning these parameters to achieve optimal performance can be challenging and time-consuming.\n",
    "4. **Evaluation Challenges:**\n",
    "    - Evaluating the performance of GANs is difficult due to the lack of standardized and universally accepted metrics. Metrics like Inception Score (IS) and Frechet Inception Distance (FID) provide some insights but have limitations and can be computationally expensive.\n",
    "5. **Mode Collapse:**\n",
    "    - Mode collapse is a common problem in GANs where the generator produces a narrow range of outputs, failing to capture the diversity of the data distribution. This can lead to poor generalization and limited applicability.\n",
    "6. **Susceptibility to Adversarial Attacks:**\n",
    "    - GANs, like other neural networks, can be vulnerable to adversarial attacks, where small perturbations to the input data can significantly impact the output, potentially leading to misleading or harmful results.\n",
    "7. **Ethical and Legal Concerns:**\n",
    "    - The ability of GANs to generate highly realistic fake data raises ethical and legal issues, particularly in the context of deepfakes and synthetic media. These technologies can be misused for malicious purposes, such as creating fake identities or spreading misinformation.\n",
    "8. **Overfitting:**\n",
    "    - GANs can sometimes overfit to the training data, especially when the training dataset is small. This can result in generated data that does not generalize well to unseen samples."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
