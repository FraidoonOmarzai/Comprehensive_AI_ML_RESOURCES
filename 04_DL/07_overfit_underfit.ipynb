{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> Overfitting And Underfitting: Regularization, Drop Out, Early Stoping </h1>\n",
    "\n",
    "![alt text](../Images/dl/underoverfit1.png)\n",
    "\n",
    "## Underfitting (High Bias)\n",
    "\n",
    "⇒ If the model is performing poorly over the train set, we call that an overfitting or high variance model.\n",
    "\n",
    "- We check the training data performance.\n",
    "\n",
    "### To Fixes:\n",
    "\n",
    "> - Try bigger network\n",
    "> \n",
    "\n",
    "> - Try getting additional features\n",
    "> \n",
    "\n",
    "> - Try adding polynomial features\n",
    "> \n",
    "\n",
    "> - Try decrease λ\n",
    "> \n",
    "\n",
    "> - Try different training layers + longer\n",
    "> \n",
    "\n",
    "> - Try different NN architecture\n",
    "> \n",
    "\n",
    "> - Better optimization algorithm (Momentum, RMSProp, Adam)\n",
    "> \n",
    "\n",
    "## Overfitting (High Variance)\n",
    "\n",
    "⇒ If the model is performing too well on the training set but performs worse over the testing set, we call underfitting or high-bias model.\n",
    "\n",
    "- We check it on the dev set performance.\n",
    "\n",
    "### To Fixes:\n",
    "\n",
    "> - Try more data\n",
    "> \n",
    "\n",
    "> - Try regularization\n",
    "> \n",
    "\n",
    "> - Try different NN architecture\n",
    "> \n",
    "\n",
    "> - Try dropout\n",
    "> \n",
    "\n",
    "> - Try early stopping\n",
    "> \n",
    "\n",
    "> - Try data augmentation\n",
    "> \n",
    "\n",
    "\n",
    "## Regularization\n",
    "\n",
    "⇒ Used to reduce overfitting by adding extra information or adding a penalty to the cost function to push weight toward zero.\n",
    "\n",
    "### Types:\n",
    "\n",
    "### 1. Lasso (L1) Regularization:\n",
    "\n",
    "- Gives penalty equal to the sum of absolute weight value\n",
    "- Robust to outliers\n",
    "- Shrink the slope to zero\n",
    "\n",
    "$$\n",
    "cost{-}fun = loss + \\lambda \\sum ||w||\n",
    "$$\n",
    "\n",
    "```\n",
    "# implementing using sklearn\n",
    "from sklearn.linear_model import Lasso\n",
    "```\n",
    "\n",
    "### 2. Ridge (L2) Regularization:\n",
    "\n",
    "- Gives penalty equal to the sum of square values of weights\n",
    "- Sensitive to outliers\n",
    "- Shrink the slope near to zero\n",
    "\n",
    "$$\n",
    "\\text{cost-fun} = loss + \\lambda \\sum ||w||^2\n",
    "$$\n",
    "\n",
    "```\n",
    "# implementing using sklearn\n",
    "from sklearn.linera_model import Ridge\n",
    "```\n",
    "\n",
    "`Note`: when we apply L1 and L2 regularization to the cost function of linear regression at the same time, it is called **Elastic Net Regression**.\n",
    "\n",
    "\n",
    "## Dropout\n",
    "\n",
    "⇒ The term “dropout” refers to dropping out the nodes (input and hidden layer) in a neural network\n",
    "\n",
    "![alt text](../Images/dl/underoverfit2.png)\n",
    "\n",
    "```\n",
    "# implementation using tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.keras.layers.Dropout(\n",
    "    rate, # Float between 0 and 1\n",
    "    noise_shape=None,\n",
    "    seed=None\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "## Early Stopping\n",
    "\n",
    "=> Early stopping is a form of regularization that stops the training process of a neural network before it reaches the maximum number of epochs or iterations. The idea is to monitor the performance of the network on a validation set, and stop the training when the validation error starts to increase or stops improving.\n",
    "\n",
    "![alt text](../Images/dl/underoverfit3.png)\n",
    "\n",
    "> Use a built-in Keras callback — [`tf.keras.callbacks.EarlyStopping`](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)—and pass it to [`Model.fit`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)\n",
    "> \n",
    "\n",
    "\n",
    "### Data Augmentation\n",
    "\n",
    "=> Overfitting occurs when a model learns the training data too well and is unable to generalize to new data. Data augmentation helps to prevent overfitting by providing the model with more data to learn from."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
