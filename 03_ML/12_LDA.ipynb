{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> LDA Algorithm In Depth </h1>\n",
    "\n",
    "![alt text](../Images/ml/LDA.png)\n",
    "\n",
    "\n",
    "- Stands for linear discriminant analysis\n",
    "- Supervised learning algorithm\n",
    "- Used for classification problems\n",
    "- Dimensionality reduction technique\n",
    "- Better having feature scaling\n",
    "- Same as PCA, but it focuses on maximizing the separability among known categories\n",
    "- It works by finding linear combinations of features that best separate two or more classes in the dataset\n",
    "\n",
    "\n",
    "**Two criteria used in LDA:**\n",
    "1. Maximize the distance between means of the two classes\n",
    "2. Minimize the variance within each class\n",
    "\n",
    "![alt text](../Images/ml/LDA1.png)\n",
    "\n",
    "\n",
    "### How does LDA work?\n",
    "\n",
    "⇒ Imagine you have a dataset where you want to classify emails as spam or not spam. LDA would analyze features like word frequency and sender information to find a linear combination (a weighted sum) of these features that best separates spam emails from legitimate ones. This essentially creates a decision boundary, like a line or plane in higher dimensions, that maximizes the separation between the classes while minimizing the overlap within each class.\n",
    "\n",
    "1. **Maximizing class separability**: LDA aims to find a projection that maximizes the separation between classes by maximizing the ratio of between-class variance to within-class variance.\n",
    "2. **Dimensionality reduction**: LDA projects the input data onto a lower-dimensional space while preserving class discriminatory information as much as possible.\n",
    "3. **Decision rule**: After dimensionality reduction, LDA applies a decision rule to assign new samples to the class with the highest posterior probability based on the class means and variances.\n",
    "\n",
    "\n",
    "### Steps involved in LDA\n",
    "\n",
    "**1. Data Preprocessing**\n",
    "\n",
    "- Gather your dataset, ensuring it has features (independent variables) and corresponding class labels (dependent variable)\n",
    "- If necessary, preprocess the data by handling missing values, normalizing or standardizing features, and splitting the dataset into training and testing sets\n",
    "\n",
    "**2. Compute Mean Vectors**\n",
    "\n",
    "- Calculate the mean vector for each class in your data. This represents the “center” of each class in the feature space\n",
    "\n",
    "**3. Compute Scatter Matrices**\n",
    "\n",
    "There are two main **scatter matrices** used in LDA:\n",
    "\n",
    "- **Within-Class Scatter Matrix (Sw):** measures the variance of the data within each class\n",
    "- **Between-Class Scatter Matrix (Sb):** measures how much the means of different classes differ from each other\n",
    "\n",
    "**4. Compute Eigenvectors and Eigenvalues**:\n",
    "\n",
    "- Calculate the eigenvectors and eigenvalues of the matrix ***Sw^(-1) * Sb***\n",
    "- The eigenvectors represent the directions (or axes) of maximum variance, while the eigenvalues represent the amount of variance explained by each eigenvector\n",
    "\n",
    "**5. Select Linear Discriminants**:\n",
    "\n",
    "- Select the top k eigenvectors (linear discriminants) corresponding to the k largest eigenvalues to form the transformation matrix W\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have already computed Sw and Sb matrices\n",
    "Sw_inverse = np.linalg.inv(Sw)\n",
    "Sw_inverse_Sb = np.dot(Sw_inverse, Sb)\n",
    "\n",
    "# Calculate eigenvectors and eigenvalues\n",
    "eigenvalues, eigenvectors = np.linalg.eig(Sw_inverse_Sb)\n",
    "\n",
    "# Sort eigenvectors based on eigenvalues\n",
    "sorted_indices = np.argsort(eigenvalues)[::-1]  # Sort indices in descending order\n",
    "sorted_eigenvalues = eigenvalues[sorted_indices]\n",
    "sorted_eigenvectors = eigenvectors[:, sorted_indices]\n",
    "\n",
    "# Select the top k eigenvectors if needed\n",
    "k = ...  # Number of discriminants to select\n",
    "selected_eigenvalues = sorted_eigenvalues[:k]\n",
    "selected_eigenvectors = sorted_eigenvectors[:, :k]\n",
    "```\n",
    "\n",
    "**6. Project Data onto Discriminant Space**:\n",
    "\n",
    "- Project the original data onto the discriminant space defined by the selected linear discriminants (W)\n",
    "- This is done by multiplying the original feature matrix by the transformation matrix W\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have already computed the eigenvectors and eigenvalues\n",
    "# selected_eigenvectors: Matrix containing selected eigenvectors as columns\n",
    "# X: Original data matrix (n_samples, n_features)\n",
    "\n",
    "# Project data onto discriminant space\n",
    "X_lda = np.dot(X, selected_eigenvectors)\n",
    "\n",
    "# X_lda now contains the data projected onto the discriminant space\n",
    "```\n",
    "\n",
    "**7. Classification or Dimensionality Reduction**:\n",
    "\n",
    "- **For classification tasks:** Apply a classifier (e.g., nearest neighbor classifier, logistic regression) on the projected data to classify new samples\n",
    "- **For dimensionality reduction:** Use the projected data with reduced dimensions for visualization or subsequent analysis\n",
    "\n",
    "\n",
    "### Advantages of LDA:\n",
    "\n",
    "- **Dimensionality reduction**: LDA reduces the dimensionality of the dataset while preserving class discriminatory information, making it useful for visualization\n",
    "- **Effective Classification:** LDA excels at finding the optimal linear separation between classes, making it a powerful tool for classification tasks\n",
    "- **Works Well with Moderate Data:** LDA performs well when you have a decent amount of labeled data, especially when the data follows Gaussian distributions\n",
    "\n",
    "### Disadvantages of LDA:\n",
    "\n",
    "- **Assumes normality**: LDA assumes that the features are normally distributed within each class, which might not hold true in practice\n",
    "- **Sensitivity to outliers**: LDA is sensitive to outliers, which can affect the estimation of class means and covariances\n",
    "- **Linear decision boundaries**: LDA assumes that the decision boundaries between classes are linear, which might not be the case for complex datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print(clf.predict([[-0.8, -1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
