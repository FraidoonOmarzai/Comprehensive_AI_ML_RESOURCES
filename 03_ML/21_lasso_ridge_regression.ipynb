{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>Lasso And Ridge Regression In Depth </h1>\n",
    "\n",
    "- Lasso and Ridge regression are two popular techniques for regularizing linear regression models, helping to prevent overfitting and improving model generalizability.\n",
    "\n",
    "![alt text](../Images/ml/lassoridge.png)\n",
    "\n",
    "## 1. Lasso Regression (L1 Regularization)\n",
    "\n",
    "- Lasso (Least Absolute Shrinkage and Selection Operator) adds a penalty equal to the sum of the value of the absolute weights\n",
    "- This penalty can shrink some coefficients to exactly zero (shrink the slope to zero), effectively performing variable selection\n",
    "- Robust to outliers\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "\n",
    "- The objective function for lasso regression is:\n",
    "\n",
    "$$\n",
    "\\text{Minimize } \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2 + \\lambda \\sum_{i=1}^n \\|w_i\\|_1 \\\\ \\text{where } |w|_1 \\text{ is the L1 norm of the coefficients} \\\\ \\text{λ is the regularization parameter}\n",
    "\n",
    "$$\n",
    "\n",
    "**Properties**:\n",
    "\n",
    "- Encourages sparsity in the coefficients, leading to simpler and more interpretable models\n",
    "- Can set some coefficients exactly to zero, thus performing feature selection\n",
    "\n",
    "**Hyperparameter**:\n",
    "\n",
    "- lambda (λ): Controls the strength of regularization. A larger λ leads to more coefficients being shrunk to zero\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- Performs automatic feature selection\n",
    "- Can handle high-dimensional data (where p>n) efficiently\n",
    "- `p` is predictors, and `n` is  number of observations\n",
    "\n",
    "**Disadvantages**:\n",
    "\n",
    "- When predictors are highly correlated, lasso tends to select one and ignore the others\n",
    "- More complex optimization problem compared to ridge regression\n",
    "\n",
    "## 2. Ridge Regression (L2 Regularization)\n",
    "\n",
    "- Ridge regression adds a penalty equal to sum of square values of weights\n",
    "- Sensitive to outliers\n",
    "- Shrink the slop near to zero\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "\n",
    "- The objective function for ridge regression is:\n",
    "    \n",
    "    $$\n",
    "    \\text{Minimize } \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2 + \\lambda \\sum_{i=1}^n ||w_i||^2_2 \\\\ \\text{where } |W|^2_2 \\text{ is the L2 norm of the coefficients} \\\\ \\text{λ is the regularization parameter}\n",
    "    $$\n",
    "    \n",
    "\n",
    "**Properties**:\n",
    "\n",
    "- Shrinks coefficients, but unlike Lasso, does not set any of them exactly to zero\n",
    "- Useful when you have multicollinearity (highly correlated predictors) because it can stabilize the solution\n",
    "- Tends to perform better when the number of predictors p is larger than the number of observations n\n",
    "\n",
    "**Hyperparameter**:\n",
    "\n",
    "- lambda (λ): Controls the strength of regularization. A larger λ implies more regularization\n",
    "\n",
    "**Advantages**:\n",
    "\n",
    "- Reduces model complexity by shrinking coefficients\n",
    "- Improves prediction accuracy by trading off a small amount of bias for a larger reduction in variance\n",
    "\n",
    "**Disadvantages**:\n",
    "\n",
    "- Does not perform feature selection (all coefficients are shrunk but none are eliminated)\n",
    "\n",
    "## 3. Elastic Net\n",
    "\n",
    "- Combines both Lasso and Ridge regression penalties\n",
    "- Useful when there are multiple correlated features.\n",
    "\n",
    "**Mathematical Formulation**:\n",
    "\n",
    "- The objective function for elastic net is:\n",
    "\n",
    "$$\n",
    "\\text{Minimize } \\frac{1}{n} \\sum_{i=1}^n (y_i - \\hat{y_i})^2 + \\lambda_1 \\sum_{i=1}^n \\|w_i\\|_1 +  \\lambda_2 \\sum_{i=1}^n \\|w_i\\|^2_2\n",
    "$$\n",
    "\n",
    "- where λ1 and λ2 are regularization parameters for L1 and L2 norms, respectively\n",
    "\n",
    "**Properties**:\n",
    "\n",
    "- Balances between the benefits of Lasso and Ridge regression\n",
    "- Can handle grouped variables and correlated features more effectively\n",
    "\n",
    "**Hyperparameters**:\n",
    "\n",
    "- λ1 and λ2: Control the strength of L1 and L2 regularization, respectively\n",
    "\n",
    "### Practical Considerations\n",
    "\n",
    "**Selecting Hyperparameters**:\n",
    "\n",
    "- Cross-validation is commonly used to select the best value of λ (and λ1, λ2 for elastic net)\n",
    "- Grid search, random search, or more advanced methods like Bayesian optimization can be employed for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "\n",
    "ridge = Ridge(alpha=1.0)  # alpha is the regularization parameter λ\n",
    "lasso = Lasso(alpha=1.0)  # alpha is the regularization parameter λ\n",
    "\n",
    "# ridge.fit(X_train, y_train)\n",
    "# lasso.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "# alpha is the overall regularization parameter, l1_ratio is the mixing parameter between Lasso and Ridge\n",
    "elastic_net = ElasticNet(alpha=1.0, l1_ratio=0.5)\n",
    "# elastic_net.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
