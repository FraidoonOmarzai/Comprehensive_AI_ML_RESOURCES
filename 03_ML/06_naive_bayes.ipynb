{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>Naive Bayes Algorithm In Depth</h1>\n",
    "\n",
    "![alt text](../Images/ml/naviepiccs.png)\n",
    "\n",
    "- Supervised Algorithm\n",
    "- Used in classification problems\n",
    "- Affected by imbalanced dataset\n",
    "- Does not require feature scaling\n",
    "- Robust to outliers\n",
    "- Can handle missing data\n",
    "- Can be heavily used for text classification and text analysis\n",
    "- Can be used with categorical and numerical data classification\n",
    "- The naive Bayes classifier is much faster with it is probability calculations\n",
    "- Naive Bayes is faster than linear models, good for very large datasets and high dimensional data, and often less accurate than linear models\n",
    "- The naive Bayes algorithm is based on the naive Bayes theorem\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac {P(B|A) P(A)}{P(B)}\\\\\n",
    "$$\n",
    "\n",
    "- Belove is how we derive Bayes' Theorem from conditional probability:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac {P(A\\cap B)}{P(B)}\\\\ P(B|A) = \\frac {P(B\\cap A)}{P(A)}\\\\ P(A\\cap B)=P(B\\cap A)\\\\P(A|B)P(B)= P(A\\cap B)\\\\P(B|A)P(A)= P(B\\cap A)\\\\P(A|B)P(B)=P(B|A)P(A)\\\\ P(A|B) = \\frac {P(B|A)P(A)}{P(B)}\n",
    "$$\n",
    "\n",
    "- P(A|B): is the probability of event A given event B (Posterior probability)\n",
    "- P(B|A): is the probability of event B given event A (Like-hood)\n",
    "- P(A): is the probabilities of event A (Prior)\n",
    "- P(B): is the probabilities of event B (Marginal)\n",
    "\n",
    "### How naive Bayes do train on the dataset:\n",
    "\n",
    "$$\n",
    " P(A|B) = \\frac {P(B|A)P(A)}{P(B)}\\\\ Dataset\\\\ X = \\{x_1,x_2,x_3...,x_n\\}, \\;\\; \\{y\\}\\\\ P(y|x_1,x_2,x_3...,x_n)=\\frac{P(x_1|y)P(x_2|y)P(x_3|y)...P(x_n|y)*P(y)}{P(x_1)P(x_2)P(x_3)...P(x_n)}\\\\ P(y|x_1,x_2,x_3...,x_n)=\\frac{P(y)\\;\\prod_{i=1}^n P(x_i|y)}{P(x_1)P(x_2)P(x_3)...P(x_n)}\n",
    "$$\n",
    "\n",
    "- X: is the set of independent features\n",
    "- y: is the set of dependent features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Numerical Example\n",
    "\n",
    "`Problem`: The below is the dataset, and we are ask to find the probability if weather is sunny whether the player should play or not.\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "- Find P(Yes | Sunny) → The probability of player play when it is sunny\n",
    "- Find P(No | Sunny) → The probability of player do not play when it is sunny\n",
    "- Compare both, the one with higher probability will be select\n",
    "\n",
    "![alt text](../Images/ml/naive1.png)\n",
    "\n",
    "#### P(Yes | Sunny):\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac {P(B|A)P(A)}{P(B)}, \\;\\;\\;\\; Baive\\; Theorem\\; Formula\\\\ So\\; we\\; can\\; write:\\;\\\\ P(Yes|Sunny) = \\frac {P(Sunny|Yes)P(Yes)}{P(Sunny)}\\\\ P(Sunny|Yes) = \\frac {3}{10} = 0.3\\\\ P(Yes) = \\frac {10}{14} = 0.71\\\\ P(Sunny) = \\frac {5}{14}=0.35 \\\\ P(Yes|Sunny) = \\frac {0.3\\times0.71}{0.35} = 0.60\n",
    "$$\n",
    "\n",
    "#### P(No| Sunny):\n",
    "\n",
    "$$\n",
    "P(No|Sunny) = \\frac {P(Sunny|No)P(No)}{P(Sunny)}\\\\ P(Sunny|No) = \\frac {2}{4} = 0.5\\\\ P(No) = \\frac {4}{14} = 0.28\\\\ P(Sunny) = \\frac {5}{14}=0.35 \\\\ P(No|Sunny) = \\frac {0.5\\times0.28}{0.35} = 0.4\n",
    "$$\n",
    "\n",
    "`Note`:As we see P(Yes|Sunny) > P(No|Sunny), so the player plays when the weather is sunny\n",
    "\n",
    "### Naive Bayes Text Classification Example\n",
    "\n",
    "Below is our dataset:\n",
    "\n",
    "![alt text](../Images/ml/text2naive.png)\n",
    "\n",
    "Before performing any operations, we need to preprocess the text data and convert our text data to numerical data.\n",
    "\n",
    "**Text Preprocessing Steps:**\n",
    "\n",
    "- Converting to lower case\n",
    "- Tokenising\n",
    "- Removing stop words\n",
    "- Words stemming\n",
    "- Removing punctuation\n",
    "- Stripping out html tags\n",
    "\n",
    "After performing text preprocessing, the next step is to convert words to vector. For the above dataset we will use bag of words (Count vectorizer) and the final dataset is: \n",
    "\n",
    "- 1 —> Positive\n",
    "- 0 —> Negative\n",
    "\n",
    "![alt text](../Images/ml/navietext.png)\n",
    "\n",
    "Now we will use the naive Bayes algorithm on the first sentence.\n",
    "\n",
    "Sentence1: The food is good → after text preprocessing: food good\n",
    "\n",
    "- food → we will call it feature x1\n",
    "- good → we will call it feature x2\n",
    "\n",
    "#### P(y=1|sentence1):\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac {P(B|A)P(A)}{P(B)}, \\;\\;\\;\\; Baive\\; Theorem\\; Formula\\\\ Based \\;on \\;the\\; above\\; formula\\; we\\; can\\; write:\\\\ P(y=1|sentence1) = P(y=1|(x_1,x_2))\\\\= \\frac {P(x_1|y=1)P(x_2|y=1)P(y=1)}{P(x_1)P(x_2)}\\\\\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(y=1)=\\frac3{5}=0.6\\\\ P(x_1|y=1)=\\frac{3}{3}=1 \\\\P(x_2|y=1)=\\frac{1}{3}=0.33 \\\\ P(x_1)=\\frac{1}{2}=0.5\\\\ P(x_2)=\\frac{1}{2}=0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    " P(y=1|(x_1,x_2)) = \\frac {1 \\times0.33 \\times0.6}{0.5 \\times 0.5} = 0.8\n",
    "$$\n",
    "\n",
    "#### P(y=0|sentence1):\n",
    "\n",
    "$$\n",
    "P(y=0|sentence1) = P(y=0|(x_1,x_2))\\\\= \\frac {P(x_1|y=0)P(x_2|y=0)P(y=0)}{P(x_1)P(x_2)}\\\\ P(y=0)=\\frac{2}{5}=0.4\\\\ P(x_1|y=0)=\\frac{2}{2}=1 \\\\P(x_2|y=0)=\\frac{0}{2}=0 \\\\ P(x_1)=\\frac{1}{2}=0.5\\\\ P(x_2)=\\frac{1}{2}=0.5\n",
    "$$\n",
    "\n",
    "$$\n",
    " P(y=0|(x_1,x_2)) = \\frac { 1\\times0 \\times0.4}{0.5 \\times 0.5} = 0\n",
    "$$\n",
    "\n",
    "`Note`: If we want to normalize those to one, we will do as below:\n",
    "\n",
    "`In probability theory, normalization often refers to scaling probabilities so that they sum up to 1. This is particularly useful when dealing with conditional probabilities or probability distributions where the total probability must equal 1`\n",
    "\n",
    "$$\n",
    "P(y=1) = \\frac {0.8}{0.8+0} = 1\\\\P(y=0) = 1 - P(y=1)=1-1=0\\\\\n",
    "$$\n",
    "\n",
    "From the above example we see that the probability of positive sentences is higher, so we can say that our sentence is positive.\n",
    "\n",
    "### **Types of Naive Bayes:**\n",
    "\n",
    "1. **Gaussian Naive Bayes**: Assumes that features follow a Gaussian (normal) distribution. It is suitable for continuous data.\n",
    "2. **Multinomial Naive Bayes**: Suitable for discrete features. It's commonly used in text classification tasks where features represent word counts or frequencies.\n",
    "3. **Bernoulli Naive Bayes**: Assumes that features are binary-valued (e.g., presence or absence of a feature).\n",
    "\n",
    "### **Advantages of Naive Bayes:**\n",
    "\n",
    "- Simple and easy to implement\n",
    "- Works well with high-dimensional data\n",
    "- Efficient and fast for training and prediction.\n",
    "- Can handle both categorical and numerical data.\n",
    "- Performs well with small datasets.\n",
    "\n",
    "### **Limitations of Naive Bayes:**\n",
    "\n",
    "- Strong feature independence assumption, which might not hold true in some cases.\n",
    "- It's known to be a bad estimator, meaning the probability outputs are not very accurate.\n",
    "- Sensitivity to the presence of irrelevant features.\n",
    "- Requires a relatively large amount of data for accurate estimation of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 2 0 2 0 1 1 1 1 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0\n",
      " 1 1 1 2 0 2 0 0 1 2 2 1 2 1 2 1 1 2 1 1 2 1 2 1 0 2 1 1 1 1 2 0 0 2 1 0 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# GaussianNB\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "gnb = GaussianNB()\n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3]\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "rng = np.random.RandomState(1)\n",
    "X = rng.randint(5, size=(6, 100))\n",
    "y = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)\n",
    "print(clf.predict(X[2:3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
