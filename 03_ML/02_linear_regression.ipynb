{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> Linear Regression </h1>\n",
    "\n",
    "- Supervised Learning ML algorithm\n",
    "- Suitable for regression problem\n",
    "- Require feature scaling (standard scaler)\n",
    "- Sensitive to outliers (getting impacted)\n",
    "\n",
    "`Note:` First algorithm to try, good for very large datasets and very high dimensional data.\n",
    "\n",
    "![alt text](../Images/ml/Linear-Regression.jpg)\n",
    "\n",
    "### 1. First, we fit the regression line.\n",
    "- We use the below equation to calculate the best line\n",
    "\n",
    "$$\n",
    "f_{w,b}(x) = wx + b\n",
    "$$\n",
    "\n",
    "- *fw,b(X):* is the predicted value\n",
    "- *w:* slope or coefficient\n",
    "- *x:* independent variable\n",
    "- *b:* constant or intercept\n",
    "\n",
    "### 2. To measure the performance of ML model we use cost function\n",
    "1. Cost function: is the average of the loss function of the entire training set\n",
    "2. Loss Function: error for a single training example \n",
    "\n",
    "![alt text](../Images/ml/linear.png)\n",
    "\n",
    "\n",
    "$$\n",
    "J(w,b) = \\frac1{2m} \\sum \\limits _{i=1} ^{m} (f_{w,b}(x)^{(i)}-y^{(i)})^{2}\n",
    "$$\n",
    "\n",
    "### 3. We use gradient descent algorithms to minimize cost function or the error between predicted and actual values in various machine learning algorithms.\n",
    "- Gradient descent is an optimization algorithm.\n",
    "- It updates the values of w and b.\n",
    "\n",
    "![alt text](../Images/ml/grad.png)\n",
    "\n",
    "$$\n",
    "w = w- \\alpha \\frac d{dw} J(w,b)\n",
    "$$\n",
    "\n",
    "$$\n",
    "b = b- \\alpha \\frac d{db} J(w,b)\n",
    "$$\n",
    "\n",
    "- In the above equation, alpha determines the step size at each iteration while moving toward a minimum loss function and it is a tuning parameter in an optimization algorithm\n",
    "- Linear regression involves convex optimization due to its convex loss function, the linear model itself is not a convex function\n",
    "- The loss function (MSE) is convex, ensuring that the optimization problem of finding the parameters (coefficients) for the linear model has only one global minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n## Implementation of linear regressin model using sklearn library\\n\\n# import the linear regression\\nfrom sklearn.linear_model import LinearRegression\\n\\n# train the model on X(independent features) and y(dependent feature)\\nreg = LinearRegression().fit(X, y)\\n\\n# check the accuracy\\nreg.score(X, y)\\n\\n# check the coefficient\\nreg.coef_\\n\\n# check the constant\\nreg.intercept_\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "## Implementation of linear regressin model using sklearn library\n",
    "\n",
    "# import the linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# train the model on X(independent features) and y(dependent feature)\n",
    "reg = LinearRegression().fit(X, y)\n",
    "\n",
    "# check the accuracy\n",
    "reg.score(X, y)\n",
    "\n",
    "# check the coefficient\n",
    "reg.coef_\n",
    "\n",
    "# check the constant\n",
    "reg.intercept_\n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
