{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) In Depth\n",
    "\n",
    "![alt text](../Images/ml/knn.png)\n",
    "\n",
    "- Supervised learning algorithm\n",
    "- Used in regression and classification problems\n",
    "- Require feature scaling\n",
    "- Can be used in text classification\n",
    "- Impacted by outliers\n",
    "- In imbalanced dataset, KNN becomes biased towards the majority instances of the training space\n",
    "- KNN is good for small dataset, good as a baseline and easy to explain\n",
    "\n",
    "## Steps In KNN\n",
    "\n",
    "1. Choose number of k neighbors\n",
    "2. Calculate the distance of the nearest neighbors\n",
    "3. Count the nearest point to each neighbors\n",
    "4. Assign the new point to the category where you counted the most neighbors(Classification)\n",
    "\n",
    "`Note`: For regression problem statement, the predicted value is given by the average of the values of it is k nearest neighbors.\n",
    "\n",
    "![alt text](../Images/ml/knn1.jpg)\n",
    "\n",
    "- We got two classes in the above picture:\n",
    "    - Male (Red)\n",
    "    - Female (Blue)\n",
    "- Green is the new point\n",
    "- K=3 for the above example\n",
    "- After calculation of nearest neighbors, We find out that two red points and one blue point are the closest\n",
    "- We assign the green point to the red due to the most counted neighbors are reds\n",
    "\n",
    "### **Key components:**\n",
    "\n",
    "- **K**: Number of nearest neighbors to consider. It's a hyperparameter that needs to be chosen beforehand\n",
    "- **Distance metric**: Commonly Euclidean distance, but other metrics like Manhattan distance, cosine similarity, etc., can also be used\n",
    "- **Decision rule**: Majority voting for classification, averaging for regression\n",
    "\n",
    "### Commonly Used Distance Metrics In KNN:\n",
    "\n",
    "1. **Euclidean Distance:** This is the most common distance metric used in KNN. It calculates the straight-line distance between two points in Euclidean space. For two points *p* and *q* in an *n* dimensional space, Euclidean distance is given by:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\sqrt{\\sum_{i=1}^{n} (p_i - q_i)^2}\n",
    "$$\n",
    "\n",
    "- d(p, q) represents the Euclidean distance between points p and q.\n",
    "- Σ (sigma) represents the sum over all dimensions (i = 1 to n).\n",
    "- p_i and q_i represent the corresponding values in the i-th dimension for points p and q, respectively.\n",
    "\n",
    "2. **Manhattan Distance (City Block or Taxicab Distance)**: It calculates the distance between two points in a grid based on the sum of the absolute differences of their coordinates. For two points *p* and *q* in an *n* dimensional space, Manhattan distance is given by:\n",
    "    \n",
    "    $$\n",
    "    d(p, q) = \\sum_{i=1}^{n} |p_i - q_i|\n",
    "    $$\n",
    "\n",
    "    We can also write the above formula for points p and q in 2D space:\n",
    "\n",
    "    **d(p, q) = |x₁ — x₂| + |y₁ — y₂|**\n",
    "\n",
    "- d(p, q): represents the Manhattan distance between points p and q\n",
    "- | |: represents the absolute value function, ensuring positive values for the distance\n",
    "- x₁ and x₂: represent the x-coordinates of points p and q, respectively\n",
    "- y₁ and y₂: represent the y-coordinates of points p and q, respectively\n",
    "    \n",
    "3. **Minkowski Distance**: This is a generalized form of both Euclidean and Manhattan distances. It is defined as:\n",
    "\n",
    "$$\n",
    "d(p, q) = \\left( \\sum_{i=1}^{n} |p_i - q_i|^r \\right)^{\\frac{1}{r}}\n",
    "$$\n",
    "\n",
    "- d(p, q): represents the Minkowski distance between points p and q\n",
    "- Σ (sigma): represents the sum over all dimensions (i = 1 to n)\n",
    "- p_i and q_i: represent the corresponding values in the i-th dimension for points p and q, respectively\n",
    "- │ │: represents the absolute value function\n",
    "- *r:* is a parameter. When *r*=1, it reduces to Manhattan distance, and when *r*=2, it reduces to Euclidean distance\n",
    "\n",
    "\n",
    "4. **Cosine Similarity**: While not a distance metric in the strictest sense, cosine similarity is often used in KNN for text data or high-dimensional data. It measures the cosine of the angle between two vectors and ranges from -1 (completely opposite) to 1 (exactly the same). It is given by:\n",
    "\n",
    "$$\n",
    "\\text{similarity}(p, q) = \\frac{p \\cdot q}{\\|p\\| \\|q\\|}\n",
    "$$\n",
    "\n",
    "- *p*⋅*q*: This represents the dot product of vectors *p* and *q*\n",
    "- ∥*p*∥: This represents the Euclidean norm (or length) of vector *p*\n",
    "- ∥q∥: This represents the Euclidean norm (or length) of vector q\n",
    "\n",
    "### **Pros:**\n",
    "\n",
    "- Simple to understand and implement\n",
    "- No training phase (lazy learner), making it efficient for online learning\n",
    "- Can handle multi-class cases effectively\n",
    "- Non-parametric, which means it can handle any type of distribution of data\n",
    "- Robust to noisy training data and effective for datasets with fewer dimensions\n",
    "\n",
    "### **Cons:**\n",
    "\n",
    "- Computationally expensive during testing, especially with large datasets, as it requires computing distances for every test point\n",
    "- Sensitive to the choice of K and the distance metric\n",
    "- Not suitable for high-dimensional data due to the curse of dimensionality (increased computational cost and decreased performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[[0.66666667 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "# KNN clssification Problem\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "neigh.fit(X, y)\n",
    "print(neigh.predict([[1.1]]))\n",
    "print(neigh.predict_proba([[0.9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5]\n"
     ]
    }
   ],
   "source": [
    "# KNN Regression Problem\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neigh = KNeighborsRegressor(n_neighbors=2)\n",
    "\n",
    "X = [[0], [1], [2], [3]]\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "neigh.fit(X, y)\n",
    "print(neigh.predict([[1.5]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
