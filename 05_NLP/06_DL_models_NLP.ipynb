{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> DL Models Used In NLP (CNN, RNNs, Seq-to-Seq, Transformers) In Depth </h1>\n",
    "\n",
    "Deep learning models have revolutionized Natural Language Processing (NLP) by enabling significant advancements in tasks such as machine translation, sentiment analysis, text generation, and more. \n",
    "\n",
    "![alt text](../Images/nlp/nlparch.png)\n",
    "\n",
    "### 1. **Convolutional Neural Networks (CNNs)**\n",
    "\n",
    "- Although CNNs are primarily used in image processing, they are also effective for certain NLP tasks such as text classification, where they capture local patterns in text.\n",
    "    - [Link For More Details](https://medium.com/python-in-plain-english/convolutional-neural-networks-cnns-in-depth-de18646af232)\n",
    "\n",
    "### 2. **Recurrent Neural Networks (RNNs)**\n",
    "\n",
    "- **Simple RNNs**: Basic form of RNNs, where the output from the previous step is fed as input to the current step.\n",
    "    - [Link For More Details](https://medium.com/@fraidoonomarzai99/recurrent-neural-networks-rnns-in-depth-6258989bfaa0)\n",
    "    \n",
    "    ![alt text](../Images/nlp/rnn1.png)\n",
    "    \n",
    "- **Long Short-Term Memory (LSTM)**: An advanced version of RNNs designed to remember information for long periods, solving the vanishing gradient problem.\n",
    "    - [Link For More Details](https://medium.com/python-in-plain-english/lstm-and-gru-in-depth-9d73915eb575)\n",
    "    \n",
    "  ![alt text](../Images/nlp/lstm.png)  \n",
    "    \n",
    "- **Gated Recurrent Unit (GRU)**: Similar to LSTMs but with a simplified architecture, often faster to train.\n",
    "    - [Link For More Details](https://medium.com/python-in-plain-english/lstm-and-gru-in-depth-9d73915eb575)\n",
    "    \n",
    "    ![alt text](../Images/nlp/GRU.png)\n",
    "    \n",
    "- **Bidirectional RNN:** Data flow in both direction, forward and backward.\n",
    "    - [Link For More Details](https://medium.com/python-in-plain-english/bidirectional-rnn-in-depth-1c6236350c34)\n",
    "    \n",
    "    \n",
    "    ![alt text](../Images/nlp/brnn2.png)\n",
    "\n",
    "### **3. Sequence-to-Sequence Models (Seq2Seq)**\n",
    "\n",
    "- Typically use encoder-decoder architectures, where the encoder processes the input sequence and the decoder generates the output sequence.\n",
    "- We have basic seq2seq and seq2seq with attention mechanism.\n",
    "- Often paired with attention mechanisms for better performance.\n",
    "    - [Link For More Details](https://medium.com/python-in-plain-english/seq2seq-encoder-decoder-and-attention-mechanism-in-depth-4e31b9af8245)\n",
    "    \n",
    "    \n",
    "    ![alt text](../Images/nlp/seq2seq2.png)\n",
    "\n",
    "### 4. **Transformers**\n",
    "\n",
    "- **Transformers**: The foundational architecture that uses self-attention mechanisms to handle long-range dependencies in text. Notable for parallelization and scalability.\n",
    "    - [Link For More Details](https://medium.com/python-in-plain-english/transformers-attention-is-all-you-need-in-depth-69cb506db9e4)\n",
    "    \n",
    "- **BERT (Bidirectional Encoder Representations from Transformers)**: Pre-trained on vast amounts of data, BERT captures context from both directions (left-to-right and right-to-left).\n",
    "- **GPT (Generative Pre-trained Transformer)**: Focuses on generating text by predicting the next word in a sentence, known for creating coherent and contextually relevant text.\n",
    "- **T5 (Text-To-Text Transfer Transformer)**: Treats every NLP task as a text-to-text problem, simplifying the framework for multiple tasks.\n",
    "\n",
    "### **Attention Mechanisms**\n",
    "\n",
    "- **Self-Attention**: Allows the model to weigh the importance of different words in a sentence when making predictions, crucial for understanding context.\n",
    "- **Global Attention**: Used to focus on all parts of the input sentence when generating each part of the output, important for translation tasks.\n",
    "\n",
    "### Most Used Models\n",
    "\n",
    "- The most widely used deep learning model in NLP today is the **Transformer** architecture, particularly its pre-trained variants. Among these, **BERT (Bidirectional Encoder Representations from Transformers)** and **GPT (Generative Pre-trained Transformer)** are the most prominent and extensively utilized.\n",
    "\n",
    "### 1. **BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "\n",
    "- **Bidirectional Context**: Unlike previous models that read text either left-to-right or right-to-left, BERT reads in both directions, providing a deeper understanding of context.\n",
    "- **Pre-training and Fine-tuning**: BERT is pre-trained on a large corpus of text and can be fine-tuned for specific tasks with relatively small datasets, making it versatile and easy to adapt to various NLP tasks.\n",
    "- **Applications**: Widely used in tasks like question answering, named entity recognition, text classification, and more.\n",
    "\n",
    "### 2. **GPT (Generative Pre-trained Transformer)**\n",
    "\n",
    "- **Autoregressive Nature**: GPT generates text by predicting the next word in a sequence, which is powerful for text generation tasks.\n",
    "- **Coherent Text Generation**: Known for its ability to generate coherent and contextually relevant text, making it ideal for applications like chatbots, content creation, and language translation.\n",
    "- **Variants**: GPT-2 and GPT-3 (and the more recent GPT-4) have significantly improved capabilities, with GPT-3 and GPT-4 being known for their ability to perform a wide variety of tasks with minimal task-specific training data.    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
