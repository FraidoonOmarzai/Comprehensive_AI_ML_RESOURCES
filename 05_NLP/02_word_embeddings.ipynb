{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> Word Embedding Techniques In Depth </h1>\n",
    "\n",
    "- It is a technique which converts words into vectors\n",
    "- Word embeddings are the richer representation of relationship between tokens\n",
    "- Word embeddings transform the high-dimensional and sparse representations of words (like one-hot encodings) into dense, lower-dimensional vectors that encode meaningful linguistic information\n",
    "\n",
    "![alt text](../Images/nlp/wordembeddings.png)\n",
    "\n",
    "## 1. Frequency-based Embeddings\n",
    "\n",
    "- Frequency-based word embeddings are a category of techniques that represent words based on their frequency of occurrence and co-occurrence in a given corpus\n",
    "- Unlike prediction-based methods that learn embeddings through prediction tasks, frequency-based methods derive embeddings from statistical properties of the text\n",
    "\n",
    "### 1. **Count Vectorization (Bag of Words)**\n",
    "\n",
    "- **Description**: This method creates a vector for each document by counting the number of times each word appears\n",
    "\n",
    "**How Bag of Words Works**\n",
    "\n",
    "1. **Tokenization**: Split the text into individual words or tokens.\n",
    "2. **Vocabulary Creation**: Create a list of all unique words (vocabulary) from the text corpus.\n",
    "3. **Vectorization**: Convert each document into a vector of numbers where each number represents the count (or frequency) of a word in the document.\n",
    "\n",
    "**Practical Example:**\n",
    "\n",
    "- Below is our text corpus:\n",
    "\n",
    "| No.  | Sentences |\n",
    "| --- | --- |\n",
    "| 1 | \"Welcome to UK, UK is a good place.” |\n",
    "| 2 | \"In the UK, Oxford Uni is the best.” |\n",
    "\n",
    "`Note`: First of all, we must perform text preprocessing, such as converting to lowercase, removing stop words and etc.\n",
    "\n",
    "- Below is our text corpus after preprocessing:\n",
    "\n",
    "| No.  | Sentences |\n",
    "| --- | --- |\n",
    "| 1 | \"welcome uk uk good place” |\n",
    "| 2 | \"uk oxford uni best” |\n",
    "\n",
    "**Step1: Tokenization**\n",
    "\n",
    "Split each sentence into words:\n",
    "\n",
    "- Sentence 1: [\"welcome\", \"uk\", \"uk\", \"good\", \"place\"]\n",
    "- Sentence 2: [\"uk\", \"oxford\", \"uni\", “best\"]\n",
    "\n",
    "**Step2: Vocabulary Creation**\n",
    "\n",
    "Create a list of unique words from the entire corpus (ignoring case and punctuation):\n",
    "\n",
    "- Vocabulary: [\"welcome\", \"uk\", \"good\", \"place\", \"oxford\", \"uni\", \"best\"]\n",
    "\n",
    "**Step3: Vectorization**\n",
    "\n",
    "Convert each sentence into a vector based on the word count:\n",
    "\n",
    "- sentence 1: [1, 2, 1, 1, 0, 0, 0]\n",
    "- sentence 2: [0, 1, 0, 0, 1, 1, 1]\n",
    "\n",
    "| sentences | welcome | uk | good | place | oxford | uni | best |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| sentence 1 | 1  | 2 | 1 | 1  | 0 | 0 | 0 |\n",
    "| sentence 2 | 0 | 1 | 0 | 0 | 1 | 1 | 1 |\n",
    "\n",
    "```python\n",
    "# python implementatoin\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\n",
    "    \"Welcome to UK, UK is a good place.\",\n",
    "    \"In the UK, Oxford Uni is the best.\"\n",
    "]\n",
    "\n",
    "# Create an instance of CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit the model and transform the corpus into BoW vectors\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Display the vocabulary and vectors\n",
    "print(\"Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"Vectors:\\n\", X.toarray())\n",
    "```\n",
    "\n",
    "- **Advantages**: Simple and easy to understand\n",
    "- **Disadvantages**: Results in high-dimensional, sparse vectors that do not capture semantic relationships between words\n",
    "\n",
    "### 2. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "\n",
    "- **Description**: TF-IDF adjusts the raw word count by considering the importance of words in the context of the entire corpus\n",
    "- **Components**:\n",
    "    - **Term Frequency (TF)**: Measures how frequently a word appears in a document\n",
    "    - **Inverse Document Frequency (IDF)**: Measures how important a word is by considering how often it appears across all documents\n",
    "- **Formula**:\n",
    "    \n",
    "    $$\n",
    "    TF-IDF=TF×IDF\\\\TF = \\frac{\\text{No of repetation of words in sentences}}{\\text{No of words in sentence}}\\\\ IDF = log( \\frac{\\text{No of sentences}}{\\text{No of sentences containing the word}})\\\\\n",
    "    $$\n",
    "    \n",
    "\n",
    "**Practical Example:**\n",
    "\n",
    "- Below is our text corpus after preprocessing:\n",
    "\n",
    "| No.  | Sentences |\n",
    "| --- | --- |\n",
    "| 1 | \"welcome uk uk good place” |\n",
    "| 2 | \"uk oxford uni best” |\n",
    "\n",
    "**Step1: Compute TF**\n",
    "\n",
    "- Vocabulary: [\"welcome\", \"uk\", \"good\", \"place\", \"oxford\", \"uni\", \"best\"]\n",
    "\n",
    "| Voc | sentence1 | sentence2 |\n",
    "| --- | --- | --- |\n",
    "| welcome | 1/5 | 0 |\n",
    "| uk | 2/5 | 1/4 |\n",
    "| good | 1/5 | 0 |\n",
    "| place | 1/5 | 0 |\n",
    "| oxford | 0 | 1/4 |\n",
    "| uni | 0 | 1/4 |\n",
    "| best | 0 | 1/4 |\n",
    "\n",
    "**Step2: Compute IDF**\n",
    "\n",
    "| Vocabulary | IDF |\n",
    "| --- | --- |\n",
    "| welcome | log(2/1) |\n",
    "| uk | log(2/3) |\n",
    "| good | log(2/1) |\n",
    "| place | log(2/1) |\n",
    "| oxford | log(2/1) |\n",
    "| uni | log(2/1) |\n",
    "| best | log(2/1) |\n",
    "\n",
    "**Step3: Compute TF x IDF**\n",
    "\n",
    "| sentences | welcome | uk | good | place | oxford | uni | best |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| sentence 1 | 1/5 x log(2/1) | 2/5 x log(2/3) | 1/5 x log(2/1) | 1/5 x log(2/1) | 0 | 0 | 0 |\n",
    "| sentence 2 | 0 | 1/4 x log(2/3) | 0 | 0 | 1/4 x log(2/1) | 1/4 x log(2/1) | 1/4 x log(2/1) |\n",
    "\n",
    "```python\n",
    "# python implementation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\n",
    "    \"Welcome to UK, UK is a good place.\",\n",
    "    \"In the UK, Oxford Uni is the best.\"\n",
    "]\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get feature names (vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense array\n",
    "tfidf_matrix = X.toarray()\n",
    "\n",
    "# Display the results\n",
    "print(\"Vocabulary:\", feature_names)\n",
    "print(\"TF-IDF Vectors:\\n\", tfidf_matrix)\n",
    "```\n",
    "\n",
    "- **Advantages**: Reduces the impact of commonly occurring words that are less informative\n",
    "- **Disadvantages**: Still results in sparse vectors and does not capture word semantics\n",
    "\n",
    "### 3. **Co-occurrence Matrices**\n",
    "\n",
    "- **Description**: Represent words by their co-occurrence with other words within a specific context window\n",
    "\n",
    "**How Co-occurrence Matrices Work**\n",
    "\n",
    "1. **Define Context Window**: Determine the window size around each word to consider its neighboring words.\n",
    "2. **Count Co-occurrences**: For each word pair within the context window, count how often they appear together.\n",
    "\n",
    "**Practical Example:**\n",
    "\n",
    "- Below is our text corpus after preprocessing:\n",
    "\n",
    "| No.  | Sentences |\n",
    "| --- | --- |\n",
    "| 1 | \"welcome uk uk good place” |\n",
    "| 2 | \"uk oxford uni best” |\n",
    "- Vocabulary: [\"welcome\", \"uk\", \"good\", \"place\", \"oxford\", \"uni\", \"best\"]\n",
    "\n",
    "**Step 1: Define Context Window Size**\n",
    "\n",
    "- A context window size of 1 (considering one word to the left and one word to the right).\n",
    "\n",
    "**Step 2: Count Co-occurrences**\n",
    "\n",
    "**1st Iteration:**\n",
    "\n",
    "- Focus word: welcome\n",
    "- Window Length = 1\n",
    "- Context words: uk → after ‘welcome’ and there is no word before\n",
    "\n",
    "**2nd Iteration:**\n",
    "\n",
    "- Focus word: uk\n",
    "- Window Length = 1\n",
    "- Context words: uk → after ‘uk’ and welcome word before ‘uk’\n",
    "\n",
    "And it continues …\n",
    "\n",
    "| _ | welcome | uk | good | place | oxford | uni | best |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| welcome | 0 | 1  | 0 | 0 | 0 | 0 | 0 |\n",
    "| uk | 1 | 2 | 1 | 0 | 1 | 0 | 0 |\n",
    "| good | 0 | 1 | 0 | 1 | 0 | 0 | 0 |\n",
    "| place | 0 | 0 | 1 | 0 | 0 | 0 | 0 |\n",
    "| oxford | 0 | 1 | 0 | 0 | 0 | 1 | 0 |\n",
    "| uni | 0 | 0 | 0 | 0 | 1 | 0 | 1 |\n",
    "| best | 0 | 0 | 0 | 0 | 0 | 1 | 0 |\n",
    "\n",
    "- **Advantages**: Captures some semantic relationships by considering word co-occurrences\n",
    "- **Disadvantages**: High-dimensional and sparse; can be computationally expensive to compute for large corpora\n",
    "\n",
    "## 2. Prediction-based Embeddings\n",
    "\n",
    "- Prediction-based word embeddings are techniques that create word representations by predicting the context in which words appear\n",
    "- These methods use neural network models to learn word vectors that capture semantic relationships between words based on their usage patterns in large text corpora\n",
    "\n",
    "### 1. **Word2Vec**\n",
    "\n",
    "Word2Vec is one of the most well-known prediction-based embedding techniques, developed at Google.\n",
    "\n",
    "- **Models**:\n",
    "    - **Continuous Bag of Words (CBOW)**: Predicts the target word based on its surrounding context words. For example, given the context words \"the quick brown\", it predicts the target word \"fox\"\n",
    "    - **Skip-gram**: Predicts the surrounding context words given the target word. For example, given the target word \"fox\", it predicts context words like \"the\", \"quick\", \"brown\"\n",
    "- **Training**: Both models use a neural network with a single hidden layer to learn embeddings. The hidden layer weights become the word vectors.\n",
    "- **Advantages**: Efficient to train on large corpora, captures semantic relationships well.\n",
    "\n",
    "### 2. **GloVe (Global Vectors for Word Representation)**\n",
    "\n",
    "GloVe, developed by researchers at Stanford, combines the benefits of both global matrix factorization (like LSA) and local context window methods (like Word2Vec).\n",
    "\n",
    "- **Method**: Constructs a word co-occurrence matrix from the corpus and then uses matrix factorization to learn word vectors. It captures the global statistical information of the corpus.\n",
    "- **Training**: Minimizes a weighted least squares objective that models the logarithm of the probability of word co-occurrences.\n",
    "- **Advantages**: Combines the strengths of word co-occurrence statistics and prediction-based learning.\n",
    "\n",
    "### 3. **FastText**\n",
    "\n",
    "Developed by Facebook's AI Research (FAIR) lab, FastText extends Word2Vec by taking sub word information into account.\n",
    "\n",
    "- **Method**: Represents words as bags of character n-grams. For example, the word \"where\" could be represented by character trigrams like \"whe\", \"her\", \"ere\".\n",
    "- **Training**: Similar to Word2Vec, but the model predicts the target word using the sum of its subword n-gram vectors.\n",
    "- **Advantages**: Handles out-of-vocabulary words better, captures morphological information.\n",
    "\n",
    "## 3. Contextualized word embeddings\n",
    "\n",
    "- Contextualized word embeddings represent a significant advancement in NLP, enabling models to understand and generate language with greater nuance and accuracy\n",
    "- By taking context into account, these embeddings provide more meaningful word representations, leading to superior performance across a wide range of NLP tasks\n",
    "\n",
    "### 1. **ELMo (Embeddings from Language Models)**\n",
    "\n",
    "**Developed by**: Allen Institute for AI\n",
    "\n",
    "**Architecture**: Deep bidirectional LSTM network\n",
    "\n",
    "**Training Objective**: Language modeling, predicting the next word in a sequence\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- **Context-Sensitive**: Generates different embeddings for the same word based on its context.\n",
    "- **Pre-trained on Large Corpus**: Can be fine-tuned for various NLP tasks, improving performance.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- **Computationally Intensive**: Requires significant computational resources for training and inference.\n",
    "- **Complexity**: More complex to implement and fine-tune compared to static embeddings.\n",
    "\n",
    "### 2. **BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "\n",
    "**Developed by**: Google AI\n",
    "\n",
    "**Architecture**: Transformer-based model\n",
    "\n",
    "**Training Objective**: Masked Language Modeling (MLM) and Next Sentence Prediction (NSP)\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- **Bidirectional Context**: Considers both left and right context, providing rich embeddings.\n",
    "- **State-of-the-Art Performance**: Excels in various NLP benchmarks and tasks.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- **Resource-Intensive**: Requires substantial memory and processing power.\n",
    "- **Training Time**: Long training times due to the complexity of the model and the amount of data required.\n",
    "\n",
    "### 3. **GPT (Generative Pretrained Transformer)**\n",
    "\n",
    "**Developed by**: OpenAI\n",
    "\n",
    "**Architecture**: Transformer decoder\n",
    "\n",
    "**Training Objective**: Language modeling, predicting the next word in a sequence\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- **Strong Performance in Text Generation**: Excels at generating coherent and contextually appropriate text.\n",
    "- **Unidirectional Context**: Effective for tasks where left-to-right context is crucial.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- **Unidirectional Limitation**: Does not consider right-to-left context, which can be a limitation for some tasks.\n",
    "- **High Computational Demand**: Similar to BERT, requires significant computational resources.\n",
    "\n",
    "### 4. **T5 (Text-To-Text Transfer Transformer)**\n",
    "\n",
    "**Developed by**: Google Research\n",
    "\n",
    "**Architecture**: Transformer architecture\n",
    "\n",
    "**Training Objective**: Converts all NLP tasks into a text-to-text format\n",
    "\n",
    "**Pros**:\n",
    "\n",
    "- **Versatility**: Can handle a wide range of NLP tasks with a unified approach.\n",
    "- **Strong Performance**: Achieves excellent results across multiple benchmarks.\n",
    "\n",
    "**Cons**:\n",
    "\n",
    "- **Resource-Intensive**: High computational requirements for training and inference.\n",
    "- **Complexity**: Implementing and fine-tuning can be challenging due to the model's flexibility and size."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
