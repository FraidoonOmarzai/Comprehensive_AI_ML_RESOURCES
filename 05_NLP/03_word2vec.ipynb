{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center>Word2Vec (CBOW, Skip-gram) In Depth</h1>\n",
    "\n",
    "- Word2Vec is an important model for natural language processing (NLP) developed by researchers at Google.\n",
    "- Word2Vec is a group of related models used to produce word embeddings, which are dense vector representations of words in a continuous vector space.\n",
    "- A two layer network to generate word embedding given a text corpus.\n",
    "- These embeddings capture semantic relationships between words based on their usage in a large corpus of text.\n",
    "\n",
    "![alt text](../Images/nlp/word2vec000.png)\n",
    "\n",
    "**Word Embeddings:** Word embeddings are fixed-size, dense vectors representing words. They capture semantic meaning in such a way that similar words have similar vectors. For example, \"king\" and \"queen\" might have vectors that are close to each other.\n",
    "\n",
    "## Architecture\n",
    "\n",
    "Word2Vec has two main architectures for generating word embeddings:\n",
    "\n",
    "1. **Continuous Bag-of-Words (CBOW)**\n",
    "2. **Skip-Gram**\n",
    "\n",
    "![alt text](../Images/nlp/word2vec11.png)\n",
    "\n",
    "### **1. Continuous Bag-of-Words (CBOW)**\n",
    "\n",
    "- Predict the target word (center word) from the surrounding context words.\n",
    "- The model averages the vectors of context words and uses this average to predict the target word.\n",
    "- Faster to train since it predicts only one word from multiple context words.\n",
    "\n",
    "### **Practical Example:**\n",
    "\n",
    "- For simplicity, imagine we got these five words: “google dream company software engineer”\n",
    "\n",
    "**1st Iteration:**\n",
    "\n",
    "- Select the window size (window_size=3).\n",
    "- The target is to predict center word from context words (Surrounding words).\n",
    "- We create the dataset where context word is independent features and center word is our output.\n",
    "\n",
    "![alt text](../Images/nlp/word2vec1.png)\n",
    "- Convert it into one hot encoding.\n",
    "\n",
    "![alt text](../Images/nlp/word2vec2.png)\n",
    "\n",
    "- Next, we pass the it to Neural Network\n",
    "\n",
    "![alt text](../Images/nlp/word2vec3.png)\n",
    "\n",
    "**2nd Iteration:**\n",
    "\n",
    "- We go for the next three words\n",
    "\n",
    "![alt text](../Images/nlp/word2vec4.png)\n",
    "\n",
    "**3rd Iteration:**\n",
    "\n",
    "- Finally, for the last three words the process is shown below:\n",
    "\n",
    "![alt text](../Images/nlp/word2vec5.png)\n",
    "\n",
    "### Getting Word Embeddings:\n",
    "\n",
    "- below is shown the process of getting word embeddings\n",
    "    \n",
    "![alt text](../Images/nlp/word2vec9.png)\n",
    "\n",
    "## **2. Skip-Gram**\n",
    "\n",
    "- Predict the surrounding context words given a target word.\n",
    "- The model takes the target word and tries to predict each of the context words within a window.\n",
    "- Performs better on smaller datasets and can capture more complex relationships between words.\n",
    "\n",
    "**Practical Example:**\n",
    "\n",
    "- We will use the above example:\n",
    "\n",
    "**1st Iteration:**\n",
    "\n",
    "- Here, our input is our output value, and we predict the surrounding words.\n",
    "\n",
    "![alt text](../Images/nlp/word2vec6.png)\n",
    "\n",
    "**2nd Iteration:**\n",
    "\n",
    "![alt text](../Images/nlp/word2vec7.png)\n",
    "\n",
    "**3rd Iteration:**\n",
    "\n",
    "![alt text](../Images/nlp/word2vec8.png)\n",
    "\n",
    "## Extensions and Alternatives\n",
    "\n",
    "Several models build on or improve Word2Vec:\n",
    "\n",
    "1. **GloVe (Global Vectors for Word Representation)**: Combines global word co-occurrence statistics with local context-based learning.\n",
    "2. **FastText**: Extends Word2Vec by representing words as n-grams of characters, improving representations for rare words.\n",
    "3. **ELMo (Embeddings from Language Models)**: Uses deep, contextualized word representations.\n",
    "4. **BERT (Bidirectional Encoder Representations from Transformers)**: Uses transformers for contextualized word embeddings, considering both left and right context.\n",
    "\n",
    "## Applications\n",
    "\n",
    "1. **Semantic Similarity**: Measuring similarity between words.\n",
    "2. **Text Classification**: Improving feature representation for classification tasks.\n",
    "3. **Machine Translation**: Enhancing translation quality by providing better word embeddings.\n",
    "4. **Information Retrieval**: Improving search results by understanding word semantics.\n",
    "5. **Recommendation Systems**: Enhancing recommendations by understanding user preferences and item descriptions.\n",
    "\n",
    "## Limitations\n",
    "\n",
    "1. **Context Independence**: It doesn't consider the order of words or their syntactic roles.\n",
    "2. **Out-of-Vocabulary Words**: It cannot handle words that were not present in the training corpus.\n",
    "3. **Fixed Embedding Size**: All words are represented by fixed-size vectors, regardless of their frequency or importance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
